{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1af7bc1-9b07-4478-a777-a55bbb0fb68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define get_transforms Function\n",
    "\n",
    "# Import necessary libraries\n",
    "from torchvision import transforms\n",
    "\n",
    "# Function to define transformations for validation dataset\n",
    "def get_transforms():\n",
    "    data_transforms = {\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize([224, 224]),  # Resize image to 224x224 pixels\n",
    "            transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "        ]),\n",
    "    }\n",
    "    return data_transforms  # Return dictionary of transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63155437-f590-4d48-b4dc-8b3e4b46d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MyResnet Class\n",
    "\n",
    "# Import necessary libraries\n",
    "from torchvision import models\n",
    "import torch\n",
    "\n",
    "# Define custom ResNet model class\n",
    "class MyResnet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load pre-trained ResNet18 model from torchvision and replace final fully connected layer\n",
    "        self.res = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.res.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.res.fc.in_features, 3),  # Replace FC layer with output size 3\n",
    "            torch.nn.ReLU()  # Apply ReLU activation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.res(x)  # Forward pass through the modified ResNet model\n",
    "        return out  # Return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a368720f-8113-4f56-8491-de3f09709bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: circle = 1.0, rectangle = 1.0, triangle = 1.0\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAIAAAD/gAIDAAAEjElEQVR4Ae2a4XLjIAyEzzd9/1fOydF1RwO2kUACzJAfGYyFpP1Y3LTN8fl8/uyXjsBfXdiOOglsWAYfbFgbloGAIXQ7a8MyEDCEbmdtWAYChtDtrA3LQMAQ+mOI1YUex3EZuMDvVYeXhjtGOTivinnm6JlWWHpGuZLXUWuCdUnqDoEpOCc7w0w9rET8HaNcZPXCPFXnmRpYLmpdknSGZf6c5SUycWKStjMFZTktrFwMqU0EK0sirD0DUvUZaGFRN8QLyBoxSW1IheTy7lRjLSxICu1+cl5aWKEy5E6EFmrcaS2spAxJ8lUleSW15rmsgQVhvrzmgXLXiQpWDkXyyu/eFXuelzmfI0fdVcFCc6yH6dCYL+muFy8UmnNgg5VrkLyWR1aGBQTMBXQAjmYwiWDcNQ288piK6oPLsC5z5VCkzvzuZZLXTVbCutR5Guz3ayZL8qqBxUTucEhedzGXrOefrIFVVHUabEWLhcBimpLXGharhMUgighOgy1ksUpYxZMoA5bhVYZ1J5Xni+YyHUlkQ1EJffi4DMurRdIPBIDilbxPnn6wWI/k9TpkKlhSodxDnrdqplW8kFLJtRjjrqw1w1gFK6JRECFGwBRRyDGnGVYijDUnk8r+aC0vp/i6DMpCXmFaWFDlLkxmpuTJpZdOlzxaWN1kzGwxAyzJS0piL8gZ0zZiITxFM5g0pYoOtsGS3bRLyjMQL4lMlpth7PDFEHcZxEs6C/jcC1kT1jjr3P3fX4+t9ZTxsoQEp1weFFbjLLRyKeOOoyaYY2QGuUrOo4eeg35fwM1VacRzDCOjd82SvJDXjAMsqUfTVoVgWgJeVKIig6axYkzNM+syaVEABfDrcnlxUq6VZ7O40DHAwVnohvSwDBpg0neAEkOOpJuzfKE8ZMNOEC96PUS633KGxUqiNVAVLkQ4omtJ4s6wZOroseTVB9mLYdFmfB32//nYgZc/LN7wDq3Dud0s5g8LGnoO+lgsBFZ/c/HGRFssBFZPTyW1Qi22GqxQi0XBGnUSYbQIi0XBQtNjB75PsUBYaHQ4L3TS+IGm6S9EjbXvIELbXUDdvOy2rkSgs+okxa2SgCQ4fcUmZ2nKcFuyUc2qiBgAqm7G8+9ZEQpdcgITZasmRWsXh+WFifcs/JnFOymbdjGLJgmKUg/chmbVQ8yazgImUu6CiQmGOwvtSgEPu9d+C4W+fvL8b8BSzgIm7FA7eplhEVjRmBhZj2OIfZaS5I41jpHW/dwljb3bWcCE/Ujk+V6Gf4KX7bI2lx9PnTGxilc6C6RcuMvtfB6/DBYwkarOpKhi12N41vv+w71C51hM7Lh3OAukKiizTpf3Th8d0KtVLWGahBRJGOMs0l+kBkbUZTEYmxE6GAOrKAmkJsHEDfc+hlSV9QNHAo7mcWsqUtTnXM6aFhPv6CywgAnWSxw3w2X5QRvUJdOhg/YKTAxhsLNAarbH06VFBjzguQ9JR44vu5xkcrCz3oKJd2uYs6j8u0hRwyNhTXK49G1sWHpW21kGVhvWhmUhYIjdz6wNy0DAELqdZYD1D7ZoDbV4BD0VAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=100x100>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Determine device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the path to the image\n",
    "image_name = \"result/random_shapes_2.png\"\n",
    "\n",
    "# Instantiate the custom ResNet model\n",
    "model = MyResnet()\n",
    "\n",
    "# Load the best trained model weights\n",
    "model.load_state_dict(torch.load('weights/multiclass_res_best_model.pt')['model_state_dict'])\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Open and load the image\n",
    "image = Image.open(image_name)\n",
    "\n",
    "# Apply transformations for validation (resize and convert to tensor)\n",
    "transform_val = get_transforms()['val']\n",
    "image_transformed = transform_val(image).unsqueeze(0).to(device)\n",
    "\n",
    "# Perform forward pass through the model and round the output to get predicted labels\n",
    "out = torch.round(model(image_transformed) * 3)\n",
    "out = out[0].tolist()\n",
    "\n",
    "# Print predicted labels for each class\n",
    "print(f'Predicted Labels: circle = {out[0]}, rectangle = {out[1]}, triangle = {out[2]}')\n",
    "\n",
    "# Display the original image\n",
    "image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f1f70-f0fa-4358-a34b-add8b63d9385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
